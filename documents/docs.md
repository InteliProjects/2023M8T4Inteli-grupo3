## Sumário

[1. Análise de négocio](#c1)

[2. Análise de Experiência do Usuário](#c2)

[3. Wireframe](#c3) 

[4. Impacto etico](#c4) 

[5. Programação](#c4)

<br>

# <a name="c1"></a> 1 Análise de négocio:

## 1.1 Matriz de risco

É uma das principais ferramentas na análise de negócios, utilizada para o gerenciamento de riscos operacionais existentes na empresa. A Figura 2, ilustra a construção da matriz de risco para o projeto.<br>

imagem 1: matriz de risco

<img width="582" alt="Captura de tela 2023-10-25 170132" src="https://github.com/2023M8T4Inteli/grupo3/assets/99191485/0b7d1e40-dbcb-4132-b74f-699718b9f87f"><br>

Fonte: Dados dos autores (2023)
Enquanto a matriz de risco proporciona uma visão clara das possíveis contingências e desafios que podem surgir, abaixo apresenta-se o plano de ação onde representa a resposta estratégica para enfrentar tais eventualidades.<br>

imagem 2: plano de ação

<img width="529" alt="Captura de tela 2023-10-25 173418" src="https://github.com/2023M8T4Inteli/grupo3/assets/99191485/fc585b45-e7d5-4292-8182-4d00c0a174f9">

Fonte: Dados dos autores (2023)

### Matriz de risco atualizada sprint 3

<img width="567" alt="Captura de tela 2023-11-27 160650" src="https://github.com/2023M8T4Inteli/grupo3/assets/99191485/ddeb8bd7-ab5a-43a5-9d4f-92224867a390">

Fonte: Dados dos autores (2023)

## <a name="c1.2"></a> 1.2 Canvas de Proposta de Valor

Ao usar o Value Proposition Canvas, as empresas podem alinhar suas ofertas com as necessidades do cliente, melhorar suas mensagens de marketing e se diferenciar de seus concorrentes.<br>

imagem 1: Canvas proposta de valor<br>
<img width="588" alt="Captura de tela 2023-10-25 173543" src="https://github.com/2023M8T4Inteli/grupo3/assets/99191485/718dd7c0-7c68-4527-8b98-d633973ad7f7"><br>
Fonte: Dados dos autores (2023)

### Pains (Dores) e Pain Relievers (Aliviadores de Dor):
As "Dores" representam os desafios específicos que consultorias enfrentam ao tentar entender o mercado na indústria alimentícia, como a demora na coleta e análise de dados, a falta de insights relevantes e a dificuldade em interpretar grandes volumes de dados brutos.
Os "Aliviadores de Dor" demonstram como o pipeline de Big Data pode solucionar esses desafios. Isso inclui um tratamento de dados mais ágil, extração de insights valiosos de vastos conjuntos de dados e a capacidade de realizar análises mais profundas e precisas.

### Gains (Ganhos) e Gain Creators (Criadores de Ganho):
Os "Ganhos" expressam os resultados positivos e benefícios que as consultorias desejam alcançar, como otimização de estratégias de "go to market", maior eficiência operacional e geração de insights inovadores para seus clientes na indústria alimentícia.
Os "Criadores de Ganho" elucidam como o pipeline de Big Data facilita esses ganhos. Isso pode incluir uma análise mais precisa dos hábitos do consumidor, tendências emergentes no mercado alimentício e identificação de oportunidades inexploradas.
Products & Services (Produtos e Serviços) e Customer Jobs (Trabalhos do Cliente):

### Conclusão
A seção "Produtos e Serviços" destaca as soluções específicas proporcionadas pelo pipeline, como ferramentas de visualização de dados, análise preditiva e segmentação avançada do mercado.
Os "Trabalhos do Cliente" representam as tarefas ou atividades que as consultorias precisam realizar, como entender padrões de consumo, identificar novos nichos de mercado e formular estratégias de penetração de mercado eficazes.
Em síntese, o canva "Proposta de Valor" para este projeto de pipeline de Big Data busca direcionar e articular o valor tangível oferecido às consultorias voltadas para a indústria alimentícia. Ele ilustra como a solução aborda dores específicas do mercado, potencializa ganhos desejados e se posiciona como uma ferramenta essencial para consultorias que buscam aprimorar suas estratégias de "go to market".

## 1.3 TAM, SAM e SOM

TAM (Total Addressable Market): Representa o mercado total que poderia se beneficiar ou necessitar do seu serviço ou produto. É o valor total caso 100% do mercado adotasse seu produto.

SAM (Serviceable Addressable Market): É a parcela do TAM que realmente pode ser alcançada por seu produto ou serviço, levando em consideração as limitações geográficas, de distribuição, capacidade e outras.

SOM (Serviceable Obtainable Market): Representa a porção do SAM que se espera atingir em um determinado período de tempo, considerando fatores como concorrência, barreiras de entrada e estratégia de implementação.

Imagem 1: TAM SAM SOM

<img width="466" alt="image" src="https://github.com/2023M8T4Inteli/grupo3/assets/99191909/68c07b84-bd2d-4a36-ac5d-2a8c3da847d3">

Fonte:Dados dos autores (2023)

TAM
Descrição: Total estimado de receita no mercado de Business Intelligence relacionado à indústria alimentícia.
Premissa: Existe uma grande demanda por insights e análises na indústria alimentícia. O TAM engloba todas as consultorias, empresas e indivíduos que poderiam potencialmente se beneficiar do uso de ferramentas de Business Intelligence na indústria alimentícia.
Valor: R$ 1,7 bilhões.

SAM
Descrição: Total estimado de receita que pode ser direcionada pelas consultorias que servem o segmento de distribuição alimentar e serviço alimentar no Brasil utilizando insights de big data.
Premissa: Dentro do amplo mercado de Business Intelligence para a indústria alimentícia, há um segmento específico focado na distribuição e no serviço alimentar que pode ser diretamente beneficiado por insights de big data. Este segmento tem necessidades mais específicas e pode ser atendido de forma mais direcionada pelo seu serviço.
Valor: R$ 150 milhões.

SOM
Descrição: Receita potencial estimada que pode ser capturada pelas consultorias em um determinado período de tempo, focando especificamente no varejo alimentar de São Paulo.
Premissa: São Paulo, sendo um grande hub comercial, possui um segmento significativo de varejo alimentar. Focar neste segmento proporciona uma oportunidade tangível e mensurável. A premissa é que, ao focar em um mercado específico e conhecido, como o varejo alimentar de São Paulo, você pode oferecer soluções mais personalizadas e alcançar uma maior penetração de mercado.
Valor: R$ 43 milhões.


# <a name="c2"> </a> 2 Análise de Experiência do usuário:

## 2.1 Personas

A persona é uma representação humanizada do público-alvo ideal e é usada para ajudar a equipe de desenvolvimento a compreender melhor suas necessidades, desejos e comportamentos. No projeto atual, foram identificadas duas personas, o tech lead, responsável pela construção do cubo de dados e o consultor de marketing, responsável pela análise do cubo.

Imagem 1: Persona, Moisés Aragão

<img width="482" alt="Captura de tela 2023-10-25 174314" src="https://github.com/2023M8T4Inteli/grupo3/assets/99191485/240567e8-16e5-4f56-b90f-ad1ab1ed8f11"><br>

Fonte: Dados dos autores (2023)

Imagem 2: Persona, Enzo Ananias

<img width="481" alt="Captura de tela 2023-10-25 174340" src="https://github.com/2023M8T4Inteli/grupo3/assets/99191485/78952910-ab05-4c30-8024-72721509cb29">

Fonte: Dados dos autores (2023)

## 2.2 Jornada do Usuário

A jornada do usuário é uma representação visual ou narrativa do percurso que um indivíduo realiza ao interagir com um produto, serviço ou sistema, desde o primeiro contato até a conclusão de um objetivo específico, levando em consideração suas emoções, experiências e desafios ao longo do caminho. Ela ajuda a compreender as necessidades, motivações e pontos de atrito do usuário, facilitando a criação de experiências mais eficientes e satisfatórias.

Imagem 1: jornada, Enzo Ananias

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/9641a0a6-767c-43ea-8da1-09c97f74092a)

Fonte: Dados dos autores (2023)

Imagem 2: jornada, Moisés Aragão

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/a20807b4-9911-4c62-8eac-023e46542b03)

Fonte: Dados dos autores (2023)

## 2.3 User Stories

Abaixo seguem quatro user stories realizados no padrão INVEST, para garantia do padrão de qualidade. Duas referentes ao teach lead e duas referentes ao consultor de marketing.

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/12e58033-a3a5-48c7-807d-cf3dcad818e5)

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/d7f689fd-39fa-48e3-9642-1c24f9ebf9c0)

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/5eadba03-541e-4a03-86c8-fe43aa9d363a)

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/6ef3ab2b-aa4a-4dfd-a8ae-aaa776954582)

# <a name="c3"> </a> 3 Wireframe

Um **wireframe de baixa fidelidade** é uma representação esquemática e simplificada de uma interface ou design de um produto digital. Ele é geralmente criado no estágio inicial do processo de design para esboçar a estrutura básica e a disposição dos elementos, sem detalhes gráficos ou estilísticos. O foco está na funcionalidade e na organização da informação, permitindo uma avaliação rápida e fácil das ideias e conceitos do design. O wireframe da sprint 1 serviu de base para a validação com o parceiro de negócios e primeiro passo para suas posteriores iterações.

## 3.1 Telas

O objetivo estimado com as interfaces do wireframe é uma visualização fidedigna dos dados provenientes do pipeline de Big Data, contribuindo na tomada de decisão dos consultores, assim como o fácil entendimento das métricas dispostas. Para que esse objetivo seja alcançado dividimos a nossa aplicação em duas telas, o *dashboard* e o *infográfico*.

### 3.1.1 Dashboard

O *Dashboard* abriga a visualização inicial, onde temos um *sidebar* para a exposição de projetos anteriores, assim o consultor tem o fácil acesso como um hub a insights, antes gerados, que poderão ser incorporados ao próximo atendimento de consultoria. Posteriormente a seleção do projeto, abriga-se a tela principal, que apresentará pelo menos 8 gráficos distintos, cada um destacando diferentes informações do mercado ou do cliente. Segmentamos a tela estrategicamente a fim de uma melhor experiência do usuário, contendo áreas destinadas somente a dados do cliente, outra a dados do mercado e região e por último a pesquisa por setor. Finalmente, a *Sidebar* disponibilizará um botão de acesso ao *Infográfico*.

A partir do gráfico 1 esperamos, uma linha temporal de vendas para a visualização de alguma tendência de longo prazo, no gráfico 2 à 4 haverão métricas de número de CNPJs, número de clientes, número de vendas realizadas e potencial do consumidor para cada canal, categoria e região. A região do gráfico 5, seguindo a proposta do Wireframe, deverá ainda ser alinhada com o cliente. No gráfico 6, por sua vez, haverá a visualização do mapa de CNPJs do mercado Brasileiro de acordo com os CNAEs desejado para análise.

![Dashboard](https://github.com/2023M8T4Inteli/grupo3/assets/99188092/14cdcd2e-27a0-4340-9b67-fe2178629d8f)

**Figura XX - Dashboard Wireframe**  
*Fonte: Elaborado pelo próprio autor (2023)*

### 3.1.2 Infográfico

A tela de *infográfico* abriga a mesma *Sidebar* presente no *dashboard*, estando na tela principal as informações que ajudarão o consultor no entendimento das métricas e tendências. Para cada gráfico, presente na tela do *dashboard*, são descritas suas métricas e entregas de valor para a análise.

![Infographic](https://github.com/2023M8T4Inteli/grupo3/assets/99188092/61eb533a-fa0d-47dd-b856-39096e677e1a)

**Figura XX - Infográfico Wireframe**  
*Fonte: Elaborado pelo próprio autor (2023)*

## 3.2 Requisitos a Cumprir

Para atender aos requisitos do cliente no desenvolvimento do cubo de dados e sua visualização no dashboard, o principal requisito é obter e integrar diversas informações essenciais do mercado e do cliente. Primeiramente, seguindo as expectativas apontadas pelo parceiro no workshop do dia 20/10/23, é necessário ter acesso ao número de CNPJs por canal e região, assim como os dados de consumo por categoria, canal e região. Além disso, a fim de permitir uma visualização regional no gráfico 6, será necessário o mapeamento entre os Canais e os códigos CNAE. Com base nesses dados, o objetivo final é criar um cubo que permita analisar o potencial de mercado de forma mais automatizada, desdobrando para cada combinação de categoria, canal e região: o número total de CNPJs, o número de clientes atendidos, as vendas realizadas e o potencial de consumo dispostos no primeiro quadrante do wireframe.

É importante ressaltar que, ao final do projeto, a entrega não consistirá em uma solução pronta, como um modelo preditivo, mas sim na estruturação e engenharia de dados necessárias para viabilizar uma solução que será implementada pela empresa no futuro. Portanto, o foco está na criação de uma infraestrutura sólida e eficiente para suportar a análise de mercado de forma automatizada e precisa.


## 3.3 Justificativa das Escolhas de Design

A justificativa das escolhas de design para essa tela é fundamentada na necessidade de proporcionar ao usuário uma experiência intuitiva e eficiente na visualização e interação com os dados apresentados. A presença de três gráficos - representando respectivamente a evolução temporal de vendas por canal, a distribuição geográfica dos canais e o relacionamento entre CNAE e CNPJ - foi determinada com base na relevância e na importância estratégica dessas informações para o usuário, de acordo com os seus desejos expressos no workshop.

Disponibilizando uma caixa de pesquisa para setores no canto inferior direito, será facilitada a navegação e a localização específica de informações para cada case de consultoria. Essa funcionalidade adiciona um componente de personalização e agilidade à experiência do usuário, permitindo-lhe encontrar dados pertinentes ao setor de atuação - Go-to-Market - do cliente específico de maneira mais granular.

Trouxemos essas escolhas de design através de técnicas que desempenham um papel crucial na criação de experiências digitais fluídas, como a antecipação de ações do usuário, que serão implementadas através de pistas visuais, animações sutis nos botões e gráficos que sugerem o próximo passo lógico:

![Dashboard_Hierarchy](https://github.com/2023M8T4Inteli/grupo3/assets/99188092/02586003-e67a-4dd4-8e50-a5002727b896)

**Figura XX - Hierarquia visual Dashboard**  
*Fonte: Elaborado pelo próprio autor (2023)*

A lógica por trás dessas disposições de elementos também está ancorada na hierarquia de informações e na facilidade de acesso às diferentes perspectivas dos dados. Sendo assim destacamos os dados mais importantes e com maior granularidade com mais proximidade do início da hierarquia visual:

![Dashboard_Hierarchy2](https://github.com/2023M8T4Inteli/grupo3/assets/99188092/9357d418-1939-4c43-b3cc-20af23bd9c71)

**Figura XX - Hierarquia visual Dashboard**  
*Fonte: Elaborado pelo próprio autor (2023)*

Enquanto que, os gráficos que apresentam menor granularidade, estão mais próximos do fim da jornada visual, como por exemplo a visualização de gráficos por setor do mercado. Fizemos essa escolha pois, refletindo na usabilidade do consultor, as métricas micro da empresa - provenientes de categoria, canal e região - podem trazer tomadas de decisão imediatas antes mesmo de se fazer uma análise macro e mais elaboradas pelas métricas de mercado/setor:

![Dashboard_Hierarchy3](https://github.com/2023M8T4Inteli/grupo3/assets/99188092/c9d57ba0-f210-42d3-b9f0-5bb09b221ea3)

**Figura XX - Hierarquia visual Dashboard**  
*Fonte: Elaborado pelo próprio autor (2023)*

A ferramenta de *grid*, que proporciona uma estrutura organizada e consistente para o layout de elementos na tela, também foi recrutada. Ao utilizar um *grid*, é possível alinhar e distribuir elementos de forma equilibrada, garantindo uma aparência profissional e coesa. Além disso, um *grid* bem elaborado facilita a adaptação do design para diferentes tamanhos de tela e dispositivos, contribuindo para uma experiência responsiva.

![Dashboard_Grid](https://github.com/2023M8T4Inteli/grupo3/assets/99188092/27cf61d1-49ee-4dde-b035-80a40b4dcdd7)

**Figura XX - Sistema Grid do Dashboard**  
*Fonte: Elaborado pelo próprio autor (2023)*

![Mobile](https://github.com/2023M8T4Inteli/grupo3/assets/99188092/24a33b9e-ec1a-4f46-9a2b-1739519c6dfa)

**Figura XX - Versão mobile Dashboard**  
*Fonte: Elaborado pelo próprio autor (2023)*

# <a name="c3"> </a> 4 Análise De etica

## Análise de impacto ético

### Análise de Impacto Social:
A incorporação de responsabilidade social em projetos empresariais é crucial para assegurar que as atividades comerciais não apenas alcancem metas econômicas, mas também contribuam positivamente para a sociedade e o meio ambiente. No caso do nosso projeto, que visa desenvolver um pipeline de Big Data para um distribuidor atuante em diversos setores, é fundamental considerar os impactos sociais envolvidos.

1. Positivos:
- Eficiência Operacional: A implementação do pipeline de Big Data pode levar a uma maior eficiência operacional para a empresa, permitindo a análise precisa do potencial de consumo em diferentes categorias e locais.
- Geração de Empregos e Parcerias: O desenvolvimento do projeto pode gerar oportunidades de emprego, especialmente se envolver a colaboração com instituições de ensino, como mencionado com os estudantes do Inteli.
- Tomada de Decisões Informada: O acesso a dados mais detalhados pode resultar em decisões mais informadas, o que, por sua vez, pode beneficiar clientes e parceiros comerciais.

2. Negativos:
- Privacidade e Segurança dos Dados: O manuseio de grandes volumes de dados requer atenção especial à privacidade e segurança. É necessário garantir que informações sensíveis estejam devidamente protegidas.
- Desigualdade de Acesso: Se o acesso ao pipeline de Big Data não for equitativo, pode haver desigualdade entre diferentes atores do setor, exacerbando disparidades já existentes.

Relação com Objetivos de Desenvolvimento Sustentável (ODS):
O projeto pode contribuir para vários Objetivos de Desenvolvimento Sustentável (ODS), como:
- ODS 8 - Trabalho Decente e Crescimento Econômico: A geração de empregos e a eficiência operacional podem contribuir para o crescimento econômico sustentável.
 - ODS 9 - Indústria, Inovação e Infraestrutura: A implementação de um pipeline de Big Data representa uma inovação na infraestrutura tecnológica.
- ODS 11 - Cidades e Comunidades Sustentáveis: A análise granular por cidade alinha-se à meta de construir cidades mais sustentáveis.

Conclusão:
Ao incorporar a responsabilidade social no desenvolvimento do pipeline de Big Data, o projeto não apenas atenderá às necessidades do cliente, mas também proporcionará benefícios tangíveis à sociedade e ao meio ambiente. A conscientização sobre os potenciais impactos positivos e negativos e a busca contínua por práticas sustentáveis são fundamentais para assegurar que o projeto esteja alinhado com os princípios da responsabilidade social corporativa.

### Viés e Discriminação:

O projeto proposto para o desenvolvimento de um pipeline de Big Data com foco em análises estatísticas para um distribuidor alimentar apresenta oportunidades significativas, mas também implica desafios relacionados a possíveis viéses algorítmicos e discriminação involuntária. É crucial considerar esses aspectos para garantir que a implementação do sistema não prejudique grupos específicos ou exclua certas categorias, canais ou regiões.

Riscos de Viés Algorítmico:

Algoritmos de análise de dados podem incorporar viés se forem treinados com conjuntos de dados que refletem desigualdades existentes. Por exemplo, se o histórico de vendas ou comportamento de consumo usado para treinar o modelo for enviesado em relação a certos grupos demográficos, o algoritmo pode perpetuar essas discrepâncias.

Discriminação e Exclusão Involuntária:

Ao analisar dados específicos de categorias, canais e regiões, há o risco de que certos grupos ou áreas sejam inadvertidamente excluídos das análises. Isso pode resultar em decisões que não representam adequadamente a diversidade do mercado ou que prejudicam involuntariamente alguns participantes.

Estratégias para Mitigar Viés e Discriminação:

1. Diversidade nos Dados de Treinamento: Garantir que os conjuntos de dados usados para treinar o modelo incluam uma representação diversificada de todas as categorias, canais e regiões relevantes. Isso ajudará a minimizar o viés algorítmico.

2. Auditoria Regular do Modelo: Implementar procedimentos regulares de auditoria para avaliar o desempenho do modelo em relação à equidade e inclusão. Identificar e corrigir possíveis viéses que surgirem durante o uso do sistema.

3. Transparência e Interpretabilidade: Tornar o processo decisório do algoritmo transparente e compreensível. Isso permitirá que os usuários entendam como as decisões são tomadas e identifiquem possíveis fontes de viés.

4. Envolvimento de Stakeholders Diversificados: Incluir stakeholders diversos, como representantes de diferentes regiões e segmentos de consumidores, no processo de desenvolvimento e validação do sistema. Isso proporcionará insights valiosos sobre a adequação e justiça das análises.

Objetivo de Desenvolvimento Responsável:
Além do objetivo principal de criar um pipeline eficiente para análises estatísticas, é crucial incluir o desenvolvimento responsável como parte integrante do projeto. Isso não apenas protegerá contra riscos éticos, mas também fortalecerá a reputação do distribuidor como uma empresa socialmente responsável.

Conclusão:

Ao abordar proativamente as questões de viés e discriminação no desenvolvimento do pipeline de Big Data, o projeto pode proporcionar não apenas benefícios operacionais para o distribuidor, mas também contribuir para práticas de negócios éticas e socialmente responsáveis.

### Privacidade:

No âmbito do projeto que visa desenvolver um pipeline de Big Data para análises estatísticas na indústria de distribuição alimentar, é imprescindível abordar a temática da privacidade e proteção de dados. Em um cenário empresarial cada vez mais orientado por informações, a coleta, armazenamento e uso de dados pessoais exigem uma atenção cuidadosa para garantir conformidade com regulamentações de privacidade e estabelecer práticas éticas. Aqui serão listadas importâncias de uma abordagem responsável acerca da privacidade de dados, reconhecendo que a confiança do cliente e a conformidade legal são pilares fundamentais para o sucesso e a replicabilidade do projeto em diferentes contextos empresariais.

Coleta de Dados:

O primeiro passo para garantir a privacidade é entender as formas de coleta de dados e, neste projeto, é essencial identificar quais tipos de dados serão coletados. No contexto de distribuição alimentar, envolve históricos de vendas, informações sobre a população, sua renda, entre outros. É crucial anonimizar quaisquer dados que possam ser considerados pessoais, como informações de contato, preferências de compra ou comportamentos individuais.

Armazenamento de Dados:

O armazenamento de dados também desempenha um papel vital na proteção da privacidade. Ao utilizar um data lake ou um data warehouse na AWS, é essencial implementar medidas de segurança robustas. Isso inclui o uso de criptografia, controle de acesso rigoroso e auditorias periódicas para garantir que os dados armazenados estejam protegidos contra acessos não autorizados. Vale lembrar que informações pessoais não anonimizadas devem ser armazenadas on premise, visto que dados sensíveis não devem ser compartilhados em nuvem para garantir seu sigilo.

Uso de Dados:

A análise estatística proposta no projeto, através do cubo OLAP, deve revelar insights valiosos, mas é crucial garantir que essas análises não comprometam a privacidade dos indivíduos. Deve-se remover informações identificáveis e garantir que os resultados agregados não possam ser rastreados até indivíduos específicos, no caso do cubo OLAP foram usados dados públicos, então eles atendem as exigências de privacidade.

Conformidade com Regulamentações:

O projeto deve aderir a regulamentações relevantes de privacidade, como a Lei Geral de Proteção de Dados (LGPD). Isso implica informar os participantes sobre a coleta de dados, obter seu consentimento quando necessário, e garantir que todos os processos estejam em conformidade com as diretrizes estabelecidas pelas autoridades de proteção de dados.

Conclusões:

Logo, integrar considerações de privacidade e proteção de dados desde o início do projeto não apenas atende a requisitos regulatórios, mas também estabelece uma base sólida para o sucesso e a aceitação do cliente. Essa abordagem ética contribui para a construção de relacionamentos duradouros e para a replicabilidade da solução em diferentes cenários de negócios.

### Equidade e justiça:

No contexto do projeto, é imperativo dedicar uma atenção especial à equidade e à justiça. Ao lidar com dados que influenciam diretamente as decisões operacionais e táticas do cliente, a abordagem deve transcender a eficácia pura e abranger a responsabilidade ética. Examina-se, portanto, a necessidade de garantir que o pipeline não apenas otimize as ações comerciais, mas também minimize disparidades, promovendo a equidade e a justiça em todas as fases do processo.

Possíveis Impactos em Grupos Específicos:

- Disparidades Regionais: Diferentes regiões podem apresentar características socioeconômicas distintas, influenciando os padrões de consumo. O pipeline de Big Data, se não considerar essas diferenças, pode resultar em estratégias desproporcionais, prejudicando ou favorecendo certas regiões em detrimento de outras.

- Viés Socioeconômico: A POF pode ter um viés socioeconômico, pois as famílias mais vulneráveis podem ter menor representatividade na pesquisa. Se o pipeline basear suas análises apenas nesses dados, pode subestimar as necessidades e padrões de consumo de grupos de menor poder aquisitivo.

-  Diferenças Regionais na Amostra: A amostra da POF pode não ser uniformemente distribuída geograficamente. Isso pode levar a uma falta de representação de certas regiões, impactando a precisão das análises para áreas específicas do país.


Minimização de Disparidades:

- Segmentação Precisa dos Dados: Ao coletar dados, é crucial segmentar de forma precisa, considerando variáveis como localização, tamanho do estabelecimento, perfil socioeconômico da região, entre outros. Isso assegura que as análises reflitam a diversidade dos contextos e evite generalizações inadequadas.

- Envolvimento Direto de Stakeholders Representativos: Garantir a participação direta de stakeholders representativos de diferentes grupos na concepção e revisão do pipeline é fundamental. Isso ajuda a validar as decisões, considerando perspectivas diversas e garantindo que as estratégias sejam justas para todos.

- Métricas de Avaliação da Equidade: Implementar métricas específicas para avaliar a equidade nas decisões e resultados do pipeline, ajustando continuamente com base nessas métricas, promove a adaptação constante e a correção de possíveis disparidades.

-  Ponderação Adequada dos Dados: Ao utilizar dados da POF, é crucial aplicar técnicas de ponderação que compensam desequilíbrios na amostra. Isso ajuda a garantir que as análises refletem com mais precisão as características demográficas e econômicas da população em diferentes regiões.
  
### Transparência e Consentimento:

A transparência e o consentimento informado são princípios fundamentais em qualquer projeto, especialmente quando se trata de lidar com dados sensíveis e estratégicos, como no caso do pipeline de Big Data proposto para o distribuidor alimentar. A garantia de que todas as partes envolvidas têm acesso claro às informações relevantes e que o consentimento é obtido de maneira adequada é crucial para o sucesso e a ética do projeto.
Transparência:

No contexto desse projeto, a transparência se refere à clareza e acessibilidade das informações relacionadas à coleta, processamento e uso dos dados. Para assegurar a transparência, é essencial que o cliente distribuidor seja plenamente informado sobre o escopo do projeto, os tipos de dados coletados, os métodos de processamento e análise estatística empregados e os resultados esperados. Isso permite que o cliente compreenda completamente como as informações serão utilizadas para atender sua demanda pelo desenvolvimento de um método de trabalho para implantação do Pipeline de Big Data.
Consentimento Informado:

O consentimento informado é a garantia de que o cliente está ciente e concorda com a coleta e o uso dos dados conforme proposto pelo projeto. No contexto deste pipeline de Big Data, o cliente distribuidor deve ser informado sobre a finalidade específica da análise estatística, os potenciais benefícios que serão obtidos e quaisquer implicações associadas à divulgação dessas informações.

Conclusões:

Logo, a responsabilidade de garantir a transparência recai à coleta e ao uso dos dados, enquanto o consentimento de dados é garantido no momento do armazenamento. Dessa forma, o uso de dados públicos garante o consentimento. Além disso, ao construir o infográfico, será necessário garantir que a visualização dos dados não possibilitará análises ambíguas.


# <a name="c4"></a> 5 Entregaveis programação:

## Sprint 1:

### Arquitetura: 

Para a execução do projeto foi necessário a definição de uma arquitetura cloud que será guia para a construção do ambiente na cloud da AWS e definição das tecnologias a serem usadas. O esquema abaixo corresponde a arquitetura realizada na Sprint 1 para apresentação e alinhamento de expectativas com os parceiros de projeto, assim também é esperado uma dinâmica de iteração durante o decorrer das Sprints.

Imagem 1: Arquitetura

![Arquitetura_Integrational drawio](https://github.com/2023M8T4Inteli/grupo3/assets/99188092/bafc4723-a3fb-4067-82f6-1b4df53aea31)

Fonte: Dados dos autores (2023)

Abaixo se encontra uma análise da arquitetura construída:

### Identificação dos dados:

#### Dados públicos: 

São fornecidos pelos canais de pesquisa do governo, tendo como mais frequente o IBGE, através de arquivos .txt em uma pasta no início do projeto. De partida, os dados não apresentavam-se estruturados, sendo fornecido um script em R para leitura desses dados e a criação de tabelas que possibilitam a compreensão. Após a leitura os dados serão convertidos para .rds e posteriormente para .csv e passados para o EC2. Nos dados públicos tambem se encaixam os dados referentes aos CNPJ no Brasil, que foram forncecidos diretamente em .csv.

Conteúdo: público, dados demográficos coletados pelo governo, que englobam fatores como condição financeira dos moradores de cada região, hábitos de consumos alimentares, despesas, aluguel dentre outros, organizados por fatores como faixa etária, faixa de renda e região. Os dados de Pesquisa de Orçamentos Familiares (POF) contemplam os seguintes tópicos:

Primeiros Resultados, que apresenta informações sobre despesas e rendimentos das famílias.
Avaliação nutricional da disponibilidade domiciliar de alimentos no Brasil.
Análise do consumo alimentar pessoal no Brasil.
Análise da Segurança Alimentar no Brasil.
Perfil das Despesas no Brasil: indicadores selecionados.
Perfil das Despesas no Brasil: indicadores selecionados de alimentação, transporte, lazer e inclusão financeira.
Perfil das Despesas no Brasil: indicadores de qualidade de vida.
Evolução dos indicadores de qualidade de vida no Brasil com base na Pesquisa de Orçamentos Familiares.

Tabelas: Os dados se distribuem em tabelas de acordo com o seu conteúdo, sendo as fornecidas pelo parceiro:

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/07c6f78d-3982-4191-a88c-557b3cd8ccc3)

#### Dados do cliente: 

Os dados referentes ao cliente serão fornecidos através de uma API hospedada pela Integration, esses dados contém informações privadas sobre os clientes da Integration, portanto, são de natureza sigilosa, não permitindo ficar em posse da Integration após o projeto. Esses dados são obtidos através de queries realizadas pela API da Integration. 

Conteúdo: Sigiloso, o conteúdo desses dados engloba informações específicas sobre o parceiro, compreendendo dados como receita gerada por um produto, receita regional, número de vendas, lista de produtos, CNPJ de clientes dentre outros. Esses dados devem variar de cliente para cliente e refletem sua situação no mercado. A solução precisa estar preparada para receber diferentes tipos de dados e estruturas, pois eles podem variar de cliente para cliente.

### Ingestão de dados:

#### Dados públicos:

Ingestão: Os dados são coletados por canais públicos como do Instituto Brasileiro de Geografia e Estatística - IBGE e outros órgãos públicos. Através de uma API, fornecida pelo governo, podem-se ser obtidos em formato .txt, sendo necessário passar por scripts em R para transformá-los em tabelas legíveis e distinguíveis. Esses dados passarão por uma pipeline em python na EC2, juntamente com os dados referentes aos CNPJ, e depois carregados em sua totalidade no S3. Após isso, os dados relevantes para os projetos atuais da empresa devem ser transferidos para o RDS, mantendo os dados mais importantes lá.  Os quais posteriormente serão  consumidos no EC2 por um código python, o qual será processado com o sparky, e também será parte do cubo de dados.

frequência: A atualização desses dados deve ser feita anualmente, acompanhando a velocidade a qual os censos são atualizados.

quantidade: Espera-se que quando atualizados será em grande quantidade, pois a frequência de atualização não será tão alta, e os dados do governo englobam milhões de pessoas.


#### Dados privados:

Já para os dados privados os dados serão ingeridos a partir da Api que fornece os dados do cliente em questão para cada projeto. A Api será consultada por um código python rodando no EC2, e posteriormente carregado, em um código python, também na EC2, utilizando sparky para uma manipulação dos dados. Eles também serão utilizados para a criação de um cubo de dados.

frequência: A atualização desses dados deve ser feita diariamente,gerenciada por um airflow para essa execução, a fim de manter os dados sempre atualizados em relação aos dados fornecidos na api do parceiro. 

quantidade: A quantidade de dados, embora grande, será recebida em um quantidade média, pois serão atualizados com uma frequência maior.


### Seleção dos serviços AWS:


#### Tratamento/processamento de dados:

EC2: realizaremos o tratamento e processamento dos dados pela máquina virtual do Amazon EC2 (Elastic Compute Cloud), através de códigos python. O EC2 é um serviço de nuvem da Amazon que oferece servidores virtuais para hospedar os recursos computacionais para a pipeline de tratamento.

Serão no total quatro códigos alocados na EC2:

Tratamento de dados públicos: Esse código receberá os dados públicos, os processará e os enviará para o AWS S3.

Consulta da api/RDS: Esse código será responsável por consultar a api do parceiro e os códigos guardados no RDS, criar um cubo de dados com essas informações e, passar as informações para o código de tratamento/transformação de dados.

Código de tratamento/ transformação de dados com sparky: Esse código será responsável pelo processamento dos tanto do cliente tanto dos dados públicos, visando alcançar as informações solicitadas pelo parceiro.

Código para o infográfico: Esse código será responsável por criar infográficos com base nos dados fornecidos, com objetivos de fornecer possíveis insights sobre os dados.

#### Armazenamento:
	
S3: Por aspectos do pipeline de big data como volume e custos, será utilizado o , serviço que permite o armazenamento de dados de forma flexível e não estruturada. Nele se espera comportar todos os dados públicos com alto volume histórico de datação prezando pela economia em espaço e processamento alcançáveis por uma base de dados não relacional. 

RDS: Posteriormente, após a seleção dos dados de interesse para as análises de mercado e negócios, será utilizado o AWS RDS para hospedar um banco de dados relacional Postgre, o qual receberá os dados de mais importância que estavam no S3. Será restrito ao último os dados desejados para a elaboração de tabelas e informações de negócios na próxima etapa de tratamento.

#### Segurança de dados:

VPC: Possibilita a criação de secções na nuvem da aws, sendo possível controlar o acesso aos serviços da AWS utilizados, criar restrições de acesso e regras de segurança. Sendo possível garantir uma segurança maior à solução.

#### Monitoramento:

Amazon Cloudwatch: Será utilizado o AWS Cloud Watch para o monitoramento dos serviços mencionados anteriormente, EC2 e RDS. Visualizando informações sobre eles e o status em tempo real.


### Fluxo dos dados:

Os dados possuem diferentes fluxos, mas se conectam futuramente no EC2, segue abaixo os fluxos dos dados até se juntarem:

Dados públicos: Os dados públicos serão retirados dos sites do governo em formato .txt, após isso os dados terão que passar por um Script R para gerar arquivos .rds a partir deles e após isso por um script python para conversão em .csv. Após isso eles serão inseridos no EC2, em um notebook que irá tratar os dados e os enviar para o S3 em sua totalidade. Os dados mais relevantes atualmente para empresa serão selecionados e enviados para o RDS e por fim enviados de volta para o EC2 para criação do cubo de dados.

Dados privados: Os dados privados serão solicitados diretamente na api fornecida pelo parceiro, esse consumo será realizado em código python localizado na EC2 e utilizados para criação do cubo de dados.

Cubo de Dados: Após a formação inicial do cubo de dados os dados passarão por outro código python no EC2, sendo processados e agregados de forma a gerar as informações que o cliente deseja para gerar um relatório. E por fim serão enviados para outro código python no EC2, o qual irá gerar infográficos a partir desses dados.

#### Segurança:

Como mencionado anteriormente, uma das ferramentas utilizadas para garantir a segurança do projeto foi o Amazon VPC, que será responsável pelo controle de acesso, criação de restrições e regras de segurança no ambiente da AWS.

Mas além dos cuidados com a segurança da AWS, também foram tomados cuidado com os dados, uma vez que apenas dados públicos serão salvos na base de dados, evitando assim que os dados privados do parceiro obtidos através da api tenham risco de vazar.


### Monitoramento e gerenciamento:

Como mencionado anteriormente na parte de serviços será utilizado o Amazon CloudWatch para o monitoramento da solução. Dentre os tópicos monitorados vale destacar:

Gasto de recursos: Para garantir que as ferramentas da AWS estejam utilizando os recursos esperados, de forma a evitar gastos indesejados.

Disponibilidade: Verificar a disponibilidade dos serviços, de forma a garantir o funcionamento da solução.

Informações: Monitoramento do que está ocorrendo nas ferramentas da solução, de forma a garantir que o fluxo esteja ocorrendo como planejado.

## Sprint 2:

### Análise Exploratória e fonte dos dados

1-Bucket base dos dados

https://basedosdados.org/dataset/49ace9c8-ae2d-454b-bed9-9b9492a3a642?table=3880670f-eceb-47ec-802b-4579ee62ae3

Importância dos dados: Os dados geográficos brasileiros foram selecionados para entender as demandas regionais.

Formato: CSV

Tamanho: base de dados não operante

Frequência de atualização: trimestralmente (2000-2021)
 
https://basedosdados.org/dataset/9fa532fb-5681-4903-b99d-01dc45fd527a?table=4b025d5a-5af0-4fa8-bd04-59de13b378ae

Importância dos dados: A Pesquisa Nacional por Amostra de Domicílios Contínua foi escolhida para entender a situação social das famílias.

Formato: CSV

Tamanho: 14 conjuntos de dados

Frequência de atualização: trimestralmente (2012-2020)


https://basedosdados.org/dataset/a1b6d2b6-4aa6-47e7-a517-8a21b28b7254?table=7b880731-ffa2-4bde-a290-ae058b3acf51

Importância dos dados: A Pesquisa de Orçamentos Familiares foi escolhida para, complementando os dados geográficos e a PNAD, viabilizar um perfil socioeconômico das famílias brasileiras.

Formato: CSV

Tamanho: 8 conjuntos de dados

Frequência de atualização: trimestralmente (2017-2018)


2-Bucket dados abertos ibge

https://dados.gov.br/dados/conjuntos-dados/io-produto-interno-bruto-dos-municipios

Importância dos dados: O PIB de cada município foi selecionado para adicionar informações sobre concentração de renda.
Formato: HTML; JSON; ODS; XML; 

Tamanho: 422 conjuntos de dados

Frequência de atualização: anualmente


https://dados.gov.br/dados/conjuntos-dados/pc-indice-nacional-de-precos-ao-consumidor-inpc

Importância dos dados: Índice Nacional de Preços ao Consumidor para demonstrar a variância dos preços.

Formato: HTML; JSON; ODS; XML; 

Tamanho: 422 conjuntos de dados

Frequência de atualização: anualmente


3-Bucket POF IBGE

https://www.ibge.gov.br/estatisticas/sociais/saude/24786-pesquisa-de-orcamentos-familiares-2.html?=&t=downloads

Importância dos dados: Pesquisa de Orçamentos Familiares dividido por décadas.

Formato: TXT

Tamanho: 8 conjuntos de dados

Frequência de atualização: anualmente (1987-1988; 1995-1996; 2002-2003; 2008-2009; 2017-2018)


4-Bucket Microdados RAIS e CAGED

https://www.gov.br/trabalho-e-emprego/pt-br/assuntos/estatisticas-trabalho/microdados-rais-e-caged

Importância dos dados: As empresas fornecem a Relação Anual de Informações Sociais e, junto com o Cadastro de Geral de Empregados e Desempregados, são fornecidas informações sobre a população economicamente ativa.

Formato: Indisponível

Tamanho: Indisponível

Frequência de atualização: Indisponível


5-Bucket Receita Federal

https://dados.gov.br/dados/conjuntos-dados/resultado-da-arrecadacao

Importância dos dados: Resultado da arrecadação com objetivo de sintetizar informações da economia brasileira.

Formato: CSV; PDF

Tamanho: 134 conjuntos de dados

Frequência de atualização: diariamente


https://dados.gov.br/dados/conjuntos-dados/repasses-da-arrecadacao-federal

Importância dos dados: Repasses da união aos estados.

Formato: CSV; PDF; XLSX

Tamanho: 134 conjuntos de dados

Frequência de atualização: diariamente


https://dados.gov.br/dados/organizacoes/visualizar/ministerio-da-fazenda

Importância dos dados: Dados relativos a título e tesouro.

Formato: CSV; PDF; XLSX; JSON

Tamanho: 134 conjuntos de dados

Frequência de atualização: diariamente


6-Bucket INEP

https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/ana

Importância dos dados: A Avaliação Nacional de Alfabetização adiciona informações sobre a educação básica da população.

Formato: CSV

Tamanho: 3,300 KB

Frequência de atualização: anualmente


https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/censo-da-educacao-superior

Importância dos dados: Evolução anual da educação brasileira (1995-2022)

Formato: CSV

Tamanho: aproximadamente 8 GB

Frequência de atualização: anualmente (1995-2022)


https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/encceja

Importância dos dados: Microdados do Exame Nacional para Certificação de Competências de Jovens e Adultos

Formato: CSV

Tamanho: 0.6 GB

Frequência de atualização: anualmente (2014, 2017-2020, 2022)

Arquivo: https://drive.google.com/drive/folders/1GDP0qcn8XWYdv0c1w46EK2eV013yXShi?usp=sharing 


7-Bucket SUS

https://opendatasus.saude.gov.br/dataset/cnes-cadastro-nacional-de-estabelecimentos-de-saude

Importância dos dados: Cadastro Nacional de Estabelecimentos de Saúde

Formato: JSON

Tamanho: 832 MB

Frequência de atualização: diariamente


8-Bucket POF_parceiro

https://drive.google.com/drive/folders/1kB92-Q_pDSlZ4YTxGy3ygxpyIKA19mwJ

Importância dos dados: Pesquisa de Orçamentos Familiares disponibilizado pelo parceiro

Formato: CSV

Tamanho: 900 MB

Frequência de atualização: anualmente


9-Bucket cnpj

https://drive.google.com/drive/folders/1hxU0AdkgvT23FRGNp4htNs45WC5mYC1S

Importância dos dados: CNPJ’s das empresas, tanto as que compram quanto as que vendem alimentos

Formato: CSV

Tamanho: 4,582 GB

Frequência de atualização: anualmente


### Data lake
#### Introdução

Um Data Lake é um sistema centralizado que permite armazenar grandes volumes de dados em seu formato natural, seja estruturado, semi-estruturado ou não estruturado. A ideia é que você pode despejar dados de diferentes fontes dentro deste lago e eles estarão disponíveis para análise e processamento com grande flexibilidade.

No nosso caso, o objetivo de criarmos uma pipeline de dados para a Integration, faz com que exista a necessidade da criação de um data lake, uma vez que estamos trabalhando com um grande volume de dados e de diferentes fontes. Para isso utilizaremos o S3.

#### S3

O S3, significa Simple Storage Solution, e a escolhemos por várias razões. Lançado em 2006, ele oferece um armazenamento de objetos dentro de uma estrutura de pastas que é extremamente eficaz em custo, começando em apenas $0,023 por gigabyte. Além disso, quanto mais você armazena, mais o custo diminui, sem limitações de capacidade.

Ele proporciona uma maneira barata e confiável de armazenar objetos com acesso de baixa latência e alta capacidade de transferência através do conteúdo do seu bucket. Um Bucket é um container da AWS S3 para armazenamento de diversas opções de arquivos. Outro ponto forte é a sua integração com serviços como SNS, SQS e Lambda, possibilitando aplicações poderosas orientadas por eventos, podendo utilizar Lambda através de um trigger. Além disso, o S3 oferece mecanismos para transferir dados antigos para armazenamento de longo prazo, reduzindo ainda mais os custos.

#### Buckets

O processo para utilizar o S3 é simples. Primeiro, cria-se um bucket. Depois, os arquivos são carregados para este bucket, com configurações personalizáveis (as quais explicamos a configuração abaixo). É importante notar que, embora o S3 utilize o termo "pastas", ele opera com uma estrutura de “flat file structure”. Isso significa que não é como as pastas do Windows; o nome do arquivo é anexado ao nome do arquivo como um prefixo, o que otimiza a organização e o acesso aos dados.

Os buckets como dito anteriormente é  um instrumento da AWS S3 que se assemelha com um container do docker. Nele você pode fazer o upload de arquivos, como JSON, PNG, HTML, CSV, etc, no caso de nosso projeto o formato escolhido foi o .csv. Se acessado na nuvem através de seu link de acesso, que pode ser definido geograficamente na própria configuração do Bucket.
Para isso criamos 10 buckets que seguem a seguinte configuração.

#### Confugração dos buckets

##### Passo 1: Configuração inicial

<img width="470" alt="image" src="https://github.com/2023M8T4Inteli/grupo3/assets/99191909/036cc8c8-7d2d-4a5b-953b-4be55ae95637">

Para começar a configuração damos o nome do bucket, (ele tem que ser único nos servidores da AWS) e a localização. Por estarmos fazendo na AWS Workbench não pudemos hospedar nosso bucket em São Paulo, mas esse seria o recomendado.

##### Passo 2: ACL

<img width="474" alt="image" src="https://github.com/2023M8T4Inteli/grupo3/assets/99191909/bff35a19-58a7-41b8-bb84-6d4e349759aa">

Desabilitamos ACLs no bucket S3 para simplificar a gestão de permissões.

##### Passo 3: Bloqueios e privacidade

<img width="469" alt="image" src="https://github.com/2023M8T4Inteli/grupo3/assets/99191909/b4684fa4-e625-483c-93c3-ff7d4e45a14f">

Para a prova de conceito, optamos por bloquear todo o acesso público ao bucket S3, garantindo segurança e simplicidade na gestão de permissões. A Integration, em um contexto de produção, deve considerar políticas específicas, possivelmente reabilitando ACLs para acesso detalhado, conforme a necessidade do cliente ou a sensibilidade dos dados.

##### Passo 4: Versionamento de bucket

<img width="472" alt="image" src="https://github.com/2023M8T4Inteli/grupo3/assets/99191909/c0fca00d-cf22-450c-aea5-b992329e0668">

Decidimos não ativar o versionamento do bucket, considerando a natureza dos dados que estamos manuseando. Como são dados atualizados anualmente, não há necessidade de manter múltiplas versões ao longo do ano, o que poderia resultar em custos desnecessários.

##### Passo 5: Criptografia
<img width="469" alt="image" src="https://github.com/2023M8T4Inteli/grupo3/assets/99191909/4898b444-cb93-4c8d-9086-3ae72cff1b67">

Para essa prova de conceito, optamos pela criptografia do lado do servidor fornecida pela própria AWS S3, a SSE-S3. Isso nos dá segurança de forma prática, sem a necessidade de gerenciar as chaves de criptografia manualmente, o que seria necessário se escolhêssemos outras opções como SSE-KMS ou DSSE-KMS.
Para a equipe da Integration, vale apontar que a escolha pelo SSE-S3 é totalmente adequada para essa fase inicial, onde estamos tratando de uma prova de conceito e de dados públicos. No entanto, quando o projeto tomar forma e começar a lidar com um volume maior de dados ou informações mais sensíveis, aí sim faria sentido pensar em algo mais robusto como o SSE-KMS ou DSSE-KMS. Essas opções trazem benefícios adicionais, como um maior controle e auditoria sobre quem está acessando as chaves de criptografia.

## Sprint 3:

### Cubo de dados e data warehouse

#### Introdução:
A criação de cubo de dados e de um data warehouse desempenha papéis cruciais em projetos de Big Data com o objetivo de estabelecer um pipeline de dados eficiente. O cubo de dados oferece a capacidade de agregar dados multidimensionalmente, melhorando o desempenho de consultas e permitindo análises detalhadas. Ele facilita a exploração de informações em diversas perspectivas e integra-se facilmente a ferramentas de Business Intelligence. Por sua vez, o data warehouse atua como um repositório centralizado, consolidando dados de várias fontes e garantindo qualidade e consistência por meio da limpeza e transformação. No contexto de um pipeline de dados, como a do nosso projeto, essa combinação proporciona agilidade na manipulação, movimentação e preparação dos dados, essenciais para análises rápidas e eficientes em ambientes de Big Data.

A criação de um cubo de dados em um data warehouse envolve entender os requisitos de negócios, extrair, transformar e carregar dados (ETL), modelar dimensões, construir o cubo e implementá-lo no data warehouse. Os usuários finais acessam o cubo através de ferramentas de Business Intelligence para análises. A manutenção contínua é essencial para ajustar o modelo conforme necessário, garantindo que os dados estejam sempre atualizados e atendam aos requisitos em evolução. Este processo fornece uma base robusta para análises eficazes.

#### Fonte de dados:

Os dados escolhidos para construção do cubo de dados são formados por dois tipos. 

Dados públicos: constituidos dos dados trabalhados anteriormente na exploratória, e tiveram suas especificações, como volume, tipo e formato trabalhados em nossa análise exploratória. Esses dados serão retirados de fontes governamentais e após passarem pelo “script para subir dados no s3” serão enviados em formato  csv para buckets no AWS S3 de acordo com suas respectivas fontes.

Dados privados(API): Esses dados são consumidos diretamente da API do parceiro, constituída de 3 tabelas Category, Establishment e Sale. E esperados que esses dados sejam consumidos periodicamente por um script python e salvos no formato csv em nosso S3 em bucket separado exclusivamente para os dados dessa API. 

Mas mesmo sendo dados provenientes de fontes diferentes todos os dados consumidos pelo Redshift foram concentrados na mesma ferramenta o AWS S3, sendo assim possível extrair todos os dados de uma mesma fonte.



#### ETL:

O processo de ETL é de extrema importância para alimentação de nossa data warehouse, uma vez que ele garante que os dados serão padronizados e armazenados de forma correta. A ETL consiste de três etapas, que serão indicadas a seguir:

Extração: Para essa etapa nossa solução realizou a extração de todos os dados trabalhados no tópico “Fonte de dados” de uma única fonte, os buckets localizados no AWS S3. para essa extração foi realizada diretamente da ferramenta do Redshift, sendo destinado a diferentes tabelas para cada tópico encontrado em nossos buckets.

Tratamento: Após a extração dos dados foi realizado o tratamento e a padronização dos mesmos. O tratamento foi feito visando formatar as tabelas em um formato padrão que possua as seguintes colunas: 

data_ingestão: Visando guardar o horário de ingestão dos dados no formato TIMESTAMP, de forma a possibilitar um controle temporal dos dados armazenados.

tag: Visando guardar o tipo/fonte desses dados de forma a ser possível os identificar de acordo com seu tipo, no formato de string.

value: Coluna responsável por guardar os JSON de cada linha dos dados consumidos no S3.
	
use: Coluna que indica qual é a utilização do dados no formato de string.

carregamento dos dados: Para essa etapa foi utilizada a ferramenta AWS Redshift para o carregamento e armazenamento dos dados, após a sua coleta no S3, os dados serão destinados a diferentes tabelas para cada tópico encontrado em nossos buckets, sendo armazenados no padrão mencionado no ETL.



#### serviço escolhido, Redshift:


Para a escolha desse serviço, foram analisadas tanto as nossas necessidades como as limitações e capacidades dos serviços analisados. 

Primeiramente vale a pena ressaltar a necessidade dos dados serem disponibilizados através de um OLAP devido a sua capacidade de processar e armazenar grandes volumes de dados. Sendo a solução escolhida para lidar com as diferentes fontes de dados e os grande volume de dados que teriam que ser armazenados e analisados frequentemente.

Sendo assim,  foi realizado o levantamento dos serviços disponíveis no ambiente da AWS, sendo cotados serviços como RDS e DynamoDB. Mas esses serviços foram eliminados de nossa seleção, uma vez que não possuíam a otimização adequada para armazenamento e disponibilização de um OLAP.

Após analisar os serviços disponíveis no ambiente AWS, o serviço escolhido foi o AWS Redshift, uma vez que este é a forma de armazenamento com a maior compatibilidade com cubos de dados OLAP. Sendo possível atender diversas necessidades que nós tínhamos com o projeto, como:

Armazenamento Colunar: O Redshift armazena dados em formato colunar, possibilitando consultas analíticas otimizadas, e a agregação e a análise de grandes volumes de dados de forma rápida e eficiente.

computação distribuída: O Redshift permite que você distribua e paralelize consultas em várias máquinas, acelerando assim as consultas realizadas.

Consultas SQL: O Redshift permite a criação e execução de consultas complexas.

Integração com Ferramentas de BI: O Redshift pode ser consumido várias ferramentas de Business Intelligence para criação de nosso infográfico.

Escalabilidade: O Redshift é um serviço que fornece alta escalabilidade, possibilitando o crescimento da solução.

Portanto devido tanto a seu suporte ao olap, quanto aos benefícios mencionados o serviço escolhido foi o AWS Redshift.


#### Estrutura e configuração do Redshift:

##### Configuração do Redshift:

Para criação de nosso datawarehouse, como mencionado no topico de escolha do serviço, foi utilizada a ferramenta  AWS Redshift, sendo realizadas diversas configurações para seu funcionamento e otimização, com intuito de maximizar a eficiência pelo menor custo o possível. Segue abaixo as estapas realizadas para criação de nosso Redshift:


1- definição do nome, e forma de configuração:

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/6bbb0971-6bc8-4a17-9229-fdbbd7bd99d4)

2- Definição de função do IAM com permissões para acesso e modificação do Redshift:

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/adf0d210-d898-4d70-bfb1-41271aed4ca4)

3- Definição de nome do grupo de trabalho, para controle do ambiente:

4- definição de RPU, para definição da capacidade de computação do Redshift:

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/218805c4-6bee-4eb2-8a19-aa02d4c7bd04)


5- Definições de VPC, para o controle de acesso e segurança da data warehause:

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/852747be-d556-439b-914f-6fe7fe4a6ff7)

6- Salvamento da configuração, e aguardar tempo de criação:

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/fe193121-bfd6-4919-9b0c-1b14ea6febb5)

##### Criação de Tabelas:

A criação das tabelas foi realizada com base nos buckets presentes no S3, que constituem nossa fonte de dados, retratada no topico anterior. Para criação das tabelas foram considerados os topicos presentes em cada bucket so S3, sendo criada uma tabela diferente para cada topico e juntando os com colunas iguais em uma unica tabela.

Para criação dessa tabelas foi utilizado o próprio Redshift, no qual foram fornecidas primeiramente informações de localização dos dados no S3, o sevidor em que estão hospedados e o formato em que os dados estão. Segue abaixo um exemplo dessa configuração:

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/fbe2f5c3-8c18-481d-8708-d5ec14b65a8b)

Após isso tambem e nescessario uma verificação dos tipos de variaveis e limitações da tabela, visando garantir que todos os dados estejam no formato e padrão desejado. Juntamente com a definição dos nomes, localização de armazenamento e a permissão de acesso da tabela, que tambem devem ser definidos nessa etapa:

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/f29b86c0-da46-4367-b2c2-10363cd1fd05)
 
Assim, sendo possível garantir que as tabelas sejam criadas no formato e organização desejada.

Criação de Views:

Além do upload dos dados para tabelas nos dados data warehause tambem foram criadas views com base nesses dados, com intuito de trazer uma nova visão sobre eles. As views foram criadas a partir da execução de querries SQL que buscam e proscessam os dados presentes em nossa tabelas no data warehouse, sendo possível juntar e processar multiplas tabelas. 


Segue um exemplo das informações que foram possiveis obter através das views criadas:

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/ba601a3b-8264-4406-909c-e7dc2ee69c55)

![image](https://github.com/2023M8T4Inteli/grupo3/assets/99202408/9aa771f5-0e8c-42a3-8a10-17dc9b688950)

Sendo assim possível, como exeplificado, obter isights como as regiões em que ocorrem a venda de certos produtos e os estabelecimentos que mais vendem.

#### Segurança dos dados:

O Redshift é um serviço de data warehouse que é totalmente gerenciado na nuvem da AWS, localização essa que pode se tornar perigosa, uma vez que fica vulnerável para acessos externos. Por essa razão, há uma grande ênfase na segurança dos dados, para que possam garantir a segurança dos dados guardados em nossa solução. Seguem alguns dos aspectos e ferramentas que destacam a segurança de dados oferecida pelo AWS Redshift.

IAM: O AWS IAM (identity and Access Manager) foi uma das ferramentas utilizadas para garantir a segurança dos dados presentes do Redshift, com ela é possível garantir permissões de controle e acesso aos usuários da AWS, controlando assim quem pode acessar os dados e fazer modificações no Redshift.

Criptografia de dados: Outra medida adotada para garantir a segurança dos dados é a criptografia desses dados para garantir sua integridade. Nesse processo é realizado o tipo de criptografia SSL (Secure Sockets Layer) na comunicação entre o S3 e o Redshift. 

Backup dos dados: Permite a criação de screenshots automáticas do cluster Redshift, que são cópias dos dados em um ponto específico no tempo que funciona para fins de recuperação. Além disso, oferece também a capacidade de fazer backups manuais e criar cópias das screenshots para maior redundância.

Logs: É capaz de gerar logs que registram informações sobre operações realizadas no cluster, incluindo também consultas e modificações na estrutura do banco de dados, garantindo o controle sobre o que está acontecendo na aplicação.

#### Monitoramento do cubo de dados(CloudWatch):

O nosso objetivo é configurar o AWS CloudWatch para que ele possa monitorar o nosso cubo de dados criado no AWS Redshift para garantir o desempenho otimizado, a disponibilidade contínua e a capacidade de resposta rápida a eventos críticos. Para garantir que essas métricas sejam cumpridas, certas ações devem ser tomadas.

Configurar Métricas do Redshift no CloudWatch:
- Habilitar a coleta de métricas no console do Redshift.
- Vincular o cluster Redshift ao CloudWatch para enviar métricas automaticamente.

Definir alarmes no CloudWatch:
-Configurar alarmes no CloudWatch para métricas críticas, como CPU, utilização de espaço, número de conexões, etc.

Logs e Análise de Desempenho:
- Configurar a captura de logs detalhados no Redshift e enviá-los para o CloudWatch.
- Alguns exemplos de logs que podem ser utilizados são:
	- Log de consulta (query):
Pode incluir detalhes como a duração da consulta, o número de linhas afetadas e o plano de execução da consulta.
	- Log de conexão (connection):
Pode incluir detalhes sobre quem está se conectando, de onde estão se conectando e quando as conexões foram estabelecidas e encerradas.
	- Log de erros (error):
Contém informações sobre erros encontrados durante a execução de consultas ou operações no cluster Redshift.
	- Log de desempenho (performance):
Pode incluir informações sobre o desempenho do sistema, como a utilização de recursos (CPU, memória, E/S) e tempos de resposta.


